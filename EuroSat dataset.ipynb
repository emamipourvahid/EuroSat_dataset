{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading models, with application to the EuroSat dataset\n",
    "\n",
    "Create a neural network that classifies land uses and land covers from satellite imagery. Save your model using Tensorflow's callbacks and reload it later.load in a pre-trained neural network classifier and compare performance with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EuroSAT overview image](data/eurosat_overview.jpg)\n",
    "\n",
    "#### The EuroSAT dataset\n",
    "\n",
    "use the [EuroSAT dataset](https://github.com/phelber/EuroSAT). It consists of 27000 labelled Sentinel-2 satellite images of different land uses: residential, industrial, highway, river, forest, pasture, herbaceous vegetation, annual crop, permanent crop and sea/lake.\n",
    "\n",
    "Construct a neural network that classifies a satellite image into one of these 10 classes, as well as applying some of the saving and loading techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data\n",
    "\n",
    "The dataset train the model on is a subset of the total data, with 4000 training images and 1000 testing images, with roughly equal numbers of each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eurosat_data():\n",
    "    data_dir = 'data/'\n",
    "    x_train = np.load(os.path.join(data_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
    "    x_test  = np.load(os.path.join(data_dir, 'x_test.npy'))\n",
    "    y_test  = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_eurosat_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_model(input_shape):\n",
    "   \n",
    "    model=Sequential([\n",
    "        Conv2D(filters=16,input_shape=input_shape,kernel_size=(3,3),activation='relu',padding='SAME',name='conv_1'),\n",
    "        Conv2D(filters=8,kernel_size=(3,3),activation='relu',padding='SAME',name='conv_2'),\n",
    "        MaxPooling2D(pool_size=(8,8),name='pool_1'),\n",
    "        Flatten(name='flatten'),\n",
    "        Dense(units=32,activation='relu',name='dense_1'),\n",
    "        Dense(units=10,activation='softmax',name='dense_2')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "model = get_new_model(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a model's test accuracy\n",
    "def get_test_accuracy(model, x_test, y_test):\n",
    "    \"\"\"Test model classification accuracy\"\"\"\n",
    "    test_loss, test_acc = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "    print('accuracy: {acc:0.3f}'.format(acc=test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv2D)              (None, 64, 64, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 64, 64, 8)         1160      \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling2D)        (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 18,354\n",
      "Trainable params: 18,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "accuracy: 0.144\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "get_test_accuracy(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create checkpoints to save model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_every_epoch():\n",
    "    return ModelCheckpoint(\n",
    "        filepath='checkpoints_every_epoch/checkpoint_{epoch:03d}',\n",
    "        frequency='epoch',\n",
    "        save_weights_only=True) \n",
    "\n",
    "\n",
    "def get_checkpoint_best_only():\n",
    "       return ModelCheckpoint(\n",
    "        filepath='checkpoints_best_only/checkpoint',\n",
    "        frequency='weights',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy'\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_early_stopping():\n",
    "    return EarlyStopping(monitor='val_accuracy',patience=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_every_epoch = get_checkpoint_every_epoch()\n",
    "checkpoint_best_only = get_checkpoint_best_only()\n",
    "early_stopping = get_early_stopping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model using the callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 1.8731 - accuracy: 0.2865 - val_loss: 1.7019 - val_accuracy: 0.3630\n",
      "Epoch 2/50\n",
      "4000/4000 [==============================] - 89s 22ms/sample - loss: 1.5372 - accuracy: 0.4235 - val_loss: 1.4482 - val_accuracy: 0.4100\n",
      "Epoch 3/50\n",
      "4000/4000 [==============================] - 86s 22ms/sample - loss: 1.3687 - accuracy: 0.4830 - val_loss: 1.3961 - val_accuracy: 0.4910\n",
      "Epoch 4/50\n",
      "4000/4000 [==============================] - 85s 21ms/sample - loss: 1.2365 - accuracy: 0.5435 - val_loss: 1.2739 - val_accuracy: 0.5230\n",
      "Epoch 5/50\n",
      "4000/4000 [==============================] - 83s 21ms/sample - loss: 1.1508 - accuracy: 0.5780 - val_loss: 1.2173 - val_accuracy: 0.5450\n",
      "Epoch 6/50\n",
      "4000/4000 [==============================] - 82s 20ms/sample - loss: 1.0949 - accuracy: 0.5987 - val_loss: 1.1527 - val_accuracy: 0.5690\n",
      "Epoch 7/50\n",
      "4000/4000 [==============================] - 82s 21ms/sample - loss: 1.0225 - accuracy: 0.6275 - val_loss: 1.0708 - val_accuracy: 0.6000\n",
      "Epoch 8/50\n",
      "4000/4000 [==============================] - 84s 21ms/sample - loss: 0.9815 - accuracy: 0.6335 - val_loss: 1.0600 - val_accuracy: 0.6230\n",
      "Epoch 9/50\n",
      "4000/4000 [==============================] - 82s 21ms/sample - loss: 0.9407 - accuracy: 0.6587 - val_loss: 1.0090 - val_accuracy: 0.6370\n",
      "Epoch 10/50\n",
      "4000/4000 [==============================] - 83s 21ms/sample - loss: 0.9117 - accuracy: 0.6725 - val_loss: 1.0028 - val_accuracy: 0.6240\n",
      "Epoch 11/50\n",
      "4000/4000 [==============================] - 85s 21ms/sample - loss: 0.8637 - accuracy: 0.6783 - val_loss: 0.9541 - val_accuracy: 0.6620\n",
      "Epoch 12/50\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.8221 - accuracy: 0.7045 - val_loss: 0.9371 - val_accuracy: 0.6530\n",
      "Epoch 13/50\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.7998 - accuracy: 0.7075 - val_loss: 0.8905 - val_accuracy: 0.6710\n",
      "Epoch 14/50\n",
      "4000/4000 [==============================] - 85s 21ms/sample - loss: 0.7732 - accuracy: 0.7195 - val_loss: 0.8495 - val_accuracy: 0.6870\n",
      "Epoch 15/50\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.7453 - accuracy: 0.7297 - val_loss: 0.8423 - val_accuracy: 0.6780\n",
      "Epoch 16/50\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.7217 - accuracy: 0.7335 - val_loss: 0.8205 - val_accuracy: 0.7040\n",
      "Epoch 17/50\n",
      "4000/4000 [==============================] - 82s 20ms/sample - loss: 0.6957 - accuracy: 0.7527 - val_loss: 0.8277 - val_accuracy: 0.6980\n",
      "Epoch 18/50\n",
      "4000/4000 [==============================] - 82s 20ms/sample - loss: 0.6736 - accuracy: 0.7588 - val_loss: 0.8545 - val_accuracy: 0.6850\n",
      "Epoch 19/50\n",
      "4000/4000 [==============================] - 85s 21ms/sample - loss: 0.6757 - accuracy: 0.7598 - val_loss: 0.8487 - val_accuracy: 0.6950\n",
      "Epoch 20/50\n",
      "4000/4000 [==============================] - 85s 21ms/sample - loss: 0.6470 - accuracy: 0.7660 - val_loss: 0.7720 - val_accuracy: 0.7160\n",
      "Epoch 21/50\n",
      "4000/4000 [==============================] - 83s 21ms/sample - loss: 0.6223 - accuracy: 0.7812 - val_loss: 0.7563 - val_accuracy: 0.7130\n",
      "Epoch 22/50\n",
      "4000/4000 [==============================] - 82s 20ms/sample - loss: 0.6138 - accuracy: 0.7768 - val_loss: 0.7896 - val_accuracy: 0.7120\n",
      "Epoch 23/50\n",
      "4000/4000 [==============================] - 82s 20ms/sample - loss: 0.5982 - accuracy: 0.7805 - val_loss: 0.8110 - val_accuracy: 0.7180\n",
      "Epoch 24/50\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.5878 - accuracy: 0.7885 - val_loss: 0.8611 - val_accuracy: 0.7030\n",
      "Epoch 25/50\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.5896 - accuracy: 0.7895 - val_loss: 0.7784 - val_accuracy: 0.7210\n",
      "Epoch 26/50\n",
      "4000/4000 [==============================] - 83s 21ms/sample - loss: 0.5524 - accuracy: 0.8043 - val_loss: 0.7683 - val_accuracy: 0.7120\n",
      "Epoch 27/50\n",
      "4000/4000 [==============================] - 82s 20ms/sample - loss: 0.5443 - accuracy: 0.8052 - val_loss: 0.7704 - val_accuracy: 0.7110\n",
      "Epoch 28/50\n",
      "4000/4000 [==============================] - 83s 21ms/sample - loss: 0.5392 - accuracy: 0.7993 - val_loss: 0.7182 - val_accuracy: 0.7390\n",
      "Epoch 29/50\n",
      "4000/4000 [==============================] - 83s 21ms/sample - loss: 0.5211 - accuracy: 0.8125 - val_loss: 0.7176 - val_accuracy: 0.7270\n",
      "Epoch 30/50\n",
      "4000/4000 [==============================] - 82s 20ms/sample - loss: 0.5203 - accuracy: 0.8127 - val_loss: 0.7239 - val_accuracy: 0.7250\n",
      "Epoch 31/50\n",
      "4000/4000 [==============================] - 84s 21ms/sample - loss: 0.5071 - accuracy: 0.8158 - val_loss: 0.7135 - val_accuracy: 0.7270\n",
      "Epoch 32/50\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.4944 - accuracy: 0.8205 - val_loss: 0.7419 - val_accuracy: 0.7330\n",
      "Epoch 33/50\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.4885 - accuracy: 0.8255 - val_loss: 0.7256 - val_accuracy: 0.7330\n",
      "Epoch 34/50\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.4657 - accuracy: 0.8310 - val_loss: 0.7013 - val_accuracy: 0.7330\n",
      "Epoch 35/50\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.4592 - accuracy: 0.8363 - val_loss: 0.7470 - val_accuracy: 0.7370\n",
      "Epoch 36/50\n",
      "4000/4000 [==============================] - 83s 21ms/sample - loss: 0.4622 - accuracy: 0.8365 - val_loss: 0.8092 - val_accuracy: 0.7190\n",
      "Epoch 37/50\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.4598 - accuracy: 0.8382 - val_loss: 0.7396 - val_accuracy: 0.7370\n",
      "Epoch 38/50\n",
      "4000/4000 [==============================] - 83s 21ms/sample - loss: 0.4330 - accuracy: 0.8470 - val_loss: 0.6931 - val_accuracy: 0.7520\n",
      "Epoch 39/50\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.4085 - accuracy: 0.8565 - val_loss: 0.7710 - val_accuracy: 0.7270\n",
      "Epoch 40/50\n",
      "4000/4000 [==============================] - 82s 21ms/sample - loss: 0.4162 - accuracy: 0.8537 - val_loss: 0.6960 - val_accuracy: 0.7580\n",
      "Epoch 41/50\n",
      "4000/4000 [==============================] - 84s 21ms/sample - loss: 0.4007 - accuracy: 0.8600 - val_loss: 0.7101 - val_accuracy: 0.7580\n",
      "Epoch 42/50\n",
      "4000/4000 [==============================] - 84s 21ms/sample - loss: 0.4075 - accuracy: 0.8577 - val_loss: 0.7358 - val_accuracy: 0.7490\n",
      "Epoch 43/50\n",
      "4000/4000 [==============================] - 84s 21ms/sample - loss: 0.4114 - accuracy: 0.8503 - val_loss: 0.7486 - val_accuracy: 0.7420\n",
      "Epoch 44/50\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.3774 - accuracy: 0.8677 - val_loss: 0.7029 - val_accuracy: 0.7560\n",
      "Epoch 45/50\n",
      "4000/4000 [==============================] - 82s 20ms/sample - loss: 0.3725 - accuracy: 0.8675 - val_loss: 0.7322 - val_accuracy: 0.7340\n",
      "Epoch 46/50\n",
      "4000/4000 [==============================] - 83s 21ms/sample - loss: 0.3625 - accuracy: 0.8740 - val_loss: 0.7590 - val_accuracy: 0.7530\n",
      "Epoch 47/50\n",
      "4000/4000 [==============================] - 83s 21ms/sample - loss: 0.3630 - accuracy: 0.8698 - val_loss: 0.7609 - val_accuracy: 0.7580\n",
      "Epoch 48/50\n",
      "4000/4000 [==============================] - 82s 20ms/sample - loss: 0.3498 - accuracy: 0.8700 - val_loss: 0.7724 - val_accuracy: 0.7530\n",
      "Epoch 49/50\n",
      "4000/4000 [==============================] - 81s 20ms/sample - loss: 0.3541 - accuracy: 0.8755 - val_loss: 0.7499 - val_accuracy: 0.7540\n",
      "Epoch 50/50\n",
      "4000/4000 [==============================] - 80s 20ms/sample - loss: 0.3322 - accuracy: 0.8825 - val_loss: 0.8119 - val_accuracy: 0.7470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f432c2f78d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [checkpoint_every_epoch, checkpoint_best_only, early_stopping]\n",
    "model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy: 88 %\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new instance of model and load on both sets of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_last_epoch(model):\n",
    "   \n",
    "    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir='checkpoints_every_epoch'))\n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_model_best_epoch(model):\n",
    "    \n",
    "    model.load_weights('checkpoints_best_only/checkpoint')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with last epoch weights:\n",
      "accuracy: 0.747\n",
      "\n",
      "Model with best epoch weights:\n",
      "accuracy: 0.758\n"
     ]
    }
   ],
   "source": [
    "model_last_epoch = get_model_last_epoch(get_new_model(x_train[0].shape))\n",
    "model_best_epoch = get_model_best_epoch(get_new_model(x_train[0].shape))\n",
    "print('Model with last epoch weights:')\n",
    "get_test_accuracy(model_last_epoch, x_test, y_test)\n",
    "print('')\n",
    "print('Model with best epoch weights:')\n",
    "get_test_accuracy(model_best_epoch, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load, from scratch, a model trained on the EuroSat dataset.\n",
    "\n",
    "Find another model trained on the `EuroSAT` dataset in `.h5` format. This model is trained on a larger subset of the EuroSAT dataset and has a more complex architecture. The path to the model is `models/EuroSatNet.h5`. See how its testing accuracy compares to my model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_eurosatnet():\n",
    "\n",
    "    model=load_model('models/EuroSatNet.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv2D)              (None, 64, 64, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 64, 64, 16)        6416      \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling2D)        (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv2D)              (None, 32, 32, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv2D)              (None, 32, 32, 16)        6416      \n",
      "_________________________________________________________________\n",
      "pool_2 (MaxPooling2D)        (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv_5 (Conv2D)              (None, 16, 16, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv_6 (Conv2D)              (None, 16, 16, 16)        6416      \n",
      "_________________________________________________________________\n",
      "pool_3 (MaxPooling2D)        (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv_7 (Conv2D)              (None, 8, 8, 16)          2320      \n",
      "_________________________________________________________________\n",
      "conv_8 (Conv2D)              (None, 8, 8, 16)          6416      \n",
      "_________________________________________________________________\n",
      "pool_4 (MaxPooling2D)        (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 41,626\n",
      "Trainable params: 41,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "accuracy: 0.810\n"
     ]
    }
   ],
   "source": [
    "model_eurosatnet = get_model_eurosatnet()\n",
    "model_eurosatnet.summary()\n",
    "get_test_accuracy(model_eurosatnet, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "tensor-flow-2-1",
   "graded_item_id": "JaRY0",
   "launcher_item_id": "mJ8fg"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
